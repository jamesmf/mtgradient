{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import typing as T\n",
    "import functools\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import pytorch_lightning as pl\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "import mtgradient\n",
    "from mtgradient import processing, datasets, models\n",
    "from mtgradient.card_initializer import featurize_cards, load_raw_card_data\n",
    "from mtgradient.constants import PlayerRank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f69e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5cfb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m pip install jupyter-black\n",
    "\n",
    "# import jupyter_black\n",
    "\n",
    "# jupyter_black.load(lab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb0954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52783f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install mlflow\n",
    "mlflow.pytorch.autolog(log_models=False, log_every_n_step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c758de",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Draft config\n",
    "# -- location of raw data\n",
    "# -- location to cache dataset\n",
    "# -- time split\n",
    "###################################\n",
    "\n",
    "# draft_csv_path = \"data/draft_data_public.NEO.PremierDraft.csv\"\n",
    "# draft_csv_path = \"data/HBG/draft_data_public.HBG.PremierDraft.csv\"\n",
    "# draft_csv_path = \"data/SNC/draft_data_public.SNC.PremierDraft.csv\"\n",
    "# draft_csv_path = \"data/DMU/draft_data_public.DMU.PremierDraft.csv\"\n",
    "# draft_csv_path = \"tests/testdata/snc_test.csv\"\n",
    "# draft_csv_path = \"tests/testdata/test_premier_draft.csv\"\n",
    "\n",
    "# cache_path = \"data/cached/neo_premier_draft\"\n",
    "# cache_path = \"data/cached/hbg_premier_draft/\"\n",
    "# cache_path = \"data/cached/snc_premier_draft\"\n",
    "# cache_path = \"data/cached/dmu_premier_draft\"\n",
    "# cache_path = \"tests/testdata/\"\n",
    "\n",
    "# test_split_cutoff = pd.Timestamp(datetime.date(2022, 3, 11))\n",
    "# test_split_cutoff = pd.Timestamp(datetime.date(2022, 7, 28))\n",
    "\n",
    "# val_split_cutoff = pd.Timestamp(datetime.date(2022, 9, 16))\n",
    "# test_split_cutoff = pd.Timestamp(datetime.date(2022, 9, 17))\n",
    "# recent_game_cutoff = val_split_cutoff - datetime.timedelta(3)\n",
    "\n",
    "# data_params = {\n",
    "#     \"data_path\": draft_csv_path,\n",
    "#     \"val_split_cutoff\": val_split_cutoff,\n",
    "#     \"test_split_cutoff\": test_split_cutoff,\n",
    "#     \"recent_game_cutoff\": recent_game_cutoff,\n",
    "# }\n",
    "\n",
    "# LOAD_CACHED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459bccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314750d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if not LOAD_CACHED:\n",
    "#     parsed_data, card_ids = processing.parse_csv(draft_csv_path, verbose=True)\n",
    "#     os.makedirs(cache_path, exist_ok=True)\n",
    "#     processing.persist_processed_dataset(cache_path, parsed_data, card_ids)\n",
    "# else:\n",
    "#     parsed_data, card_ids = processing.load_processed_dataset(cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cf952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft_times = [[v[\"draft_time\"]] for k, v in parsed_data.items()]\n",
    "# px.histogram(pd.DataFrame(draft_times, columns=[\"t\"]), x=\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0364c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_subset = {\n",
    "#     k: v\n",
    "#     for k, v in parsed_data.items()\n",
    "#     if v.get(\"draft_time\", datetime.date(1999, 9, 9)) < test_split_cutoff\n",
    "# }\n",
    "# val_subset = {\n",
    "#     k: v\n",
    "#     for k, v in parsed_data.items()\n",
    "#     if v.get(\"draft_time\", datetime.date(1999, 9, 9)) >= val_split_cutoff\n",
    "#     and v.get(\"draft_time\", datetime.date(1999, 9, 9)) < test_split_cutoff\n",
    "# }\n",
    "# test_subset = {\n",
    "#     k: v\n",
    "#     for k, v in parsed_data.items()\n",
    "#     if v.get(\"draft_time\", datetime.date(1999, 9, 9)) >= test_split_cutoff\n",
    "# }\n",
    "\n",
    "# train_draft_dataset = datasets.DraftDataset(train_subset, recent_game_cutoff)\n",
    "# val_draft_dataset = datasets.DraftDataset(val_subset, val_split_cutoff, use_all=True)\n",
    "# test_draft_dataset = datasets.DraftDataset(test_subset, test_split_cutoff, use_all=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdac2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat = torch.utils.data.ConcatDataset([train_draft_dataset, train_draft_dataset2])\n",
    "# itr = iter(concat)\n",
    "\n",
    "# tdl = torch.utils.data.DataLoader(\n",
    "#     concat,\n",
    "#     batch_size=20,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     collate_fn=models.collate_batch,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=False,\n",
    "# )\n",
    "# next(iter(tdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afa072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_config = [\n",
    "#     {\n",
    "#         \"set_id\": \"DMU\",\n",
    "#         \"datasets\": [\n",
    "#             {\n",
    "#                 \"format\": \"premier\",\n",
    "#                 \"format_id_int\": 0,\n",
    "#                 \"file\": \"data/DMU/draft_data_public.DMU.PremierDraft.csv\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"format\": \"traditional\",\n",
    "#                 \"format_id_int\": 1,\n",
    "#                 \"file\": \"data/DMU/draft_data_public.DMU.TradDraft.csv\",\n",
    "#                 \"training_only\": True,\n",
    "#             },\n",
    "#         ],\n",
    "#         \"set_id_int\": 0,\n",
    "#     },\n",
    "#     {\n",
    "#         \"set_id\": \"SNC\",\n",
    "#         \"datasets\": [\n",
    "#             {\n",
    "#                 \"format\": \"premier\",\n",
    "#                 \"format_id_int\": 0,\n",
    "#                 \"file\": \"data/SNC/draft_data_public.SNC.PremierDraft.csv\",\n",
    "#                 \"training_only\": True,\n",
    "#             },\n",
    "#         ],\n",
    "#         \"set_id_int\": 1,\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# dataset_config = [\n",
    "#     {\n",
    "#         \"set_id\": \"MID\",\n",
    "#         \"datasets\": [\n",
    "#             {\n",
    "#                 \"format\": \"premier\",\n",
    "#                 \"format_id_int\": 0,\n",
    "#                 \"file\": \"data/MID/draft_data_public.MID.PremierDraft.csv\",\n",
    "#             },\n",
    "#         ],\n",
    "#         \"set_id_int\": 0,\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "\n",
    "# dataset_config = [\n",
    "#     {\n",
    "#         \"set_id\": \"VOW\",\n",
    "#         \"datasets\": [\n",
    "#             {\n",
    "#                 \"format\": \"premier\",\n",
    "#                 \"format_id_int\": 0,\n",
    "#                 \"file\": \"data/VOW/draft_data_public.VOW.PremierDraft.csv\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"format\": \"quick\",\n",
    "#                 \"format_id_int\": 1,\n",
    "#                 \"file\": \"data/VOW/draft_data_public.VOW.QuickDraft.csv\",\n",
    "#                 \"training_only\": True,\n",
    "#             },\n",
    "#         ],\n",
    "#         \"set_id_int\": 0,\n",
    "#     },\n",
    "#     #         {\n",
    "#     #         \"set_id\": \"SNC\",\n",
    "#     #         \"datasets\": [\n",
    "#     #             {\n",
    "#     #                 \"format\": \"premier\",\n",
    "#     #                 \"format_id_int\": 0,\n",
    "#     #                 \"file\": \"data/SNC/draft_data_public.SNC.PremierDraft.csv\",\n",
    "#     #                 \"training_only\": True,\n",
    "#     #             },\n",
    "#     #         ],\n",
    "#     #         \"set_id_int\": 1,\n",
    "#     #     },\n",
    "# ]\n",
    "\n",
    "\n",
    "# dataset_config = [\n",
    "#     {\n",
    "#         \"set_id\": \"NEO\",\n",
    "#         \"datasets\": [\n",
    "#             {\n",
    "#                 \"format\": \"premier\",\n",
    "#                 \"format_id_int\": 0,\n",
    "#                 \"file\": \"data/NEO/draft_data_public.NEO.PremierDraft.csv\",\n",
    "#             }\n",
    "#         ],\n",
    "#         \"set_id_int\": 0,\n",
    "#     },\n",
    "    #         {\n",
    "    #         \"set_id\": \"SNC\",\n",
    "    #         \"datasets\": [\n",
    "    #             {\n",
    "    #                 \"format\": \"premier\",\n",
    "    #                 \"format_id_int\": 0,\n",
    "    #                 \"file\": \"data/SNC/draft_data_public.SNC.PremierDraft.csv\",\n",
    "    #                 \"training_only\": True,\n",
    "    #             },\n",
    "    #         ],\n",
    "    #         \"set_id_int\": 1,\n",
    "    #     },\n",
    "# ]\n",
    "\n",
    "\n",
    "dataset_config = [\n",
    "    {\n",
    "        \"set_id\": \"ONE\",\n",
    "        \"datasets\": [\n",
    "            {\n",
    "                \"format\": \"premier\",\n",
    "                \"format_id_int\": 0,\n",
    "                \"file\": \"data/ONE/draft_data_public.ONE.PremierDraft.csv\",\n",
    "            }\n",
    "        ],\n",
    "        \"set_id_int\": 0,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "sets_params = {\"set_names\": \",\".join([i[\"set_id\"] for i in dataset_config])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d32800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_df = pd.read_csv(\n",
    "#     \"data/NEO/draft_data_public.NEO.PremierDraft.csv\", usecols=[\"draft_time\"]\n",
    "# )\n",
    "\n",
    "# px.histogram(date_df, x=\"draft_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175430e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check = datasets.MultiDataset(dataset_config)\n",
    "\n",
    "train_draft_dataset = torch.utils.data.ConcatDataset(check.train_datasets)\n",
    "val_draft_dataset = torch.utils.data.ConcatDataset(check.val_datasets)\n",
    "test_draft_dataset = torch.utils.data.ConcatDataset(check.test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454cfa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_ids = check.card_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b938856",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_draft_dataset), len(val_draft_dataset), len(test_draft_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/cached/one_only.pkl\", \"wb\") as f:\n",
    "    pickle.dump(check, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ba4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/cached/one_only.pkl\", \"rb\") as f:\n",
    "    check = pickle.load(f)\n",
    "card_ids = check.card_ids\n",
    "train_draft_dataset = torch.utils.data.ConcatDataset(check.train_datasets)\n",
    "val_draft_dataset = torch.utils.data.ConcatDataset(check.val_datasets)\n",
    "test_draft_dataset = torch.utils.data.ConcatDataset(check.test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45979a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_out = [draft[\"draft_time\"] for draft in check.train_datasets[0].dataset.values()]\n",
    "# df_out = pd.DataFrame(data_out, columns=[\"draft_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57845e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = ([draft[\"draft_time\"] for draft in check.train_datasets[0].dataset.values()] +\n",
    "    [draft[\"draft_time\"] for draft in check.val_datasets[0].dataset.values()] +\n",
    "           [draft[\"draft_time\"] for draft in check.test_datasets[0].dataset.values()]   \n",
    "           )\n",
    "df_out = pd.DataFrame(data_out, columns=[\"draft_time\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df_out, x=\"draft_time\", cumulative=True, histnorm=\"percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11257b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the model and ensure its forward method works\n",
    "N_CARDS = 1000\n",
    "EMB_DIM = 512\n",
    "\n",
    "params = {\n",
    "    \"n_cards\": N_CARDS,\n",
    "    \"emb_dim\": EMB_DIM,\n",
    "    \"n_cards_in_pack\": 16,\n",
    "    \"n_steps\": 30000,\n",
    "}\n",
    "model = models.DraftTransformer(\n",
    "    **params,\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "\n",
    "card_data_df = load_raw_card_data(\n",
    "    \"data/oracle-cards-20230619090235.json\", \"data/\", refresh=False\n",
    ")\n",
    "feature_vectors = featurize_cards(card_data_df, card_ids, N_CARDS, EMB_DIM)\n",
    "model.init_embedding(feature_vectors)\n",
    "\n",
    "print(\n",
    "    model(\n",
    "        models.collate_batch([next(iter(train_draft_dataset))], device=\"cuda:0\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80efdbba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mlflow.pytorch.autolog(log_models=False, log_every_n_step=100)\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except Exception as e:\n",
    "    pass\n",
    "mlflow.start_run()\n",
    "mlflow.log_params(sets_params)\n",
    "# mlflow.log_params({k: str(v) for k, v in params.items()})\n",
    "# mlflow.log_params({k: str(v) for k, v in data_params.items()})\n",
    "# mlflow.log_metrics(\n",
    "#     {\"len_val_subset\": len(val_subset), \"len_train_subset\": len(train_subset)}\n",
    "# )\n",
    "\n",
    "# swa_args = dict(swa_lrs=5e-6, annealing_epochs=10, swa_epoch_start=25)\n",
    "# mlflow.log_params({k: str(v) for k, v in swa_args.items()})\n",
    "\n",
    "# swa = pl.callbacks.StochasticWeightAveraging(**swa_args)\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(save_weights_only=True, filename=\"model\")\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_steps=model.n_steps,\n",
    "    #     profiler=\"simple\",\n",
    "    precision=16,\n",
    "    #     callbacks=[checkpoint, swa],\n",
    "    callbacks=[checkpoint],\n",
    "    #     benchmark=True,\n",
    "    check_val_every_n_epoch=1,\n",
    "    #     limit_val_batches=1,\n",
    "    #     overfit_batches=1,\n",
    ")\n",
    "\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_draft_dataset,\n",
    "    batch_size=200,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=models.collate_batch,\n",
    "    num_workers=2,\n",
    "    pin_memory=False,\n",
    ")\n",
    "val_dl = torch.utils.data.DataLoader(\n",
    "    val_draft_dataset,\n",
    "    batch_size=200,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=models.collate_batch,\n",
    "    num_workers=2,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa90c91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dp = checkpoint.dirpath\n",
    "fn = os.listdir(trainer.checkpoint_callback.dirpath)[0]\n",
    "checkpoint_path = os.path.join(dp, fn)\n",
    "\n",
    "# log our model weights\n",
    "mlflow.log_artifact(checkpoint_path)\n",
    "\n",
    "# log the card ids\n",
    "card_id_path = f\"{dp}/card_ids.json\"\n",
    "with open(card_id_path, \"w\") as f:\n",
    "    json.dump(card_ids, f)\n",
    "\n",
    "mlflow.log_artifact(card_id_path)\n",
    "\n",
    "# log the dataset config\n",
    "ds_config_path = f\"{dp}/dataset_config.json\"\n",
    "with open(ds_config_path, \"w\") as f:\n",
    "    json.dump(dataset_config, f)\n",
    "mlflow.log_artifact(ds_config_path)\n",
    "\n",
    "# log the model params/config\n",
    "model_config_path = f\"{dp}/model_config.json\"\n",
    "with open(model_config_path, \"w\") as f:\n",
    "    json.dump(params, f)\n",
    "\n",
    "mlflow.log_artifact(model_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c52910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f24847",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = mlflow.active_run()\n",
    "run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4732d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf artifacts/SNC/\n",
    "# ! cp -r mlruns/0/6f8fd2c4b705471791a1010c8b06d2e7/artifacts/ artifacts/SNC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2849b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b6440",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with open(\"artifacts/SNC/model_config.json\", \"r\") as f:\n",
    "#     m_config = json.load(f)\n",
    "# loaded = models.DraftTransformer.load_from_checkpoint(\n",
    "#     \"artifacts/SNC/model-v1.ckpt\", **m_config\n",
    "# )\n",
    "# t = pl.Trainer(\n",
    "#     gpus=[0],\n",
    "# )\n",
    "\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "    test_draft_dataset,\n",
    "    batch_size=200,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=models.collate_batch,\n",
    "    num_workers=2,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.test(model, test_dataloaders=[test_dl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9135fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "by_rank = pd.DataFrame(model.test_data, columns=[\"rank_ind\", \"pick_loss\", \"maindeck_rate_loss\"])\n",
    "by_rank[\"rank_ind\"] = by_rank[\"rank_ind\"].apply(int)\n",
    "by_rank[\"rank\"] = by_rank[\"rank_ind\"].apply(lambda x: PlayerRank(x).name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eaa577",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_rank.groupby([\"rank_ind\", \"rank\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(\n",
    "    by_rank.sort_values(\"rank_ind\"),\n",
    "    x=\"rank\",\n",
    "    y=\"pick_loss\",\n",
    "    range_y=(0, 1.25),\n",
    "    title=\"Cross entropy loss for card pick by rank\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(\n",
    "    by_rank.sort_values(\"rank_ind\"),\n",
    "    x=\"rank\",\n",
    "    y=\"maindeck_rate_loss\",\n",
    "    range_y=(0, 1.0),\n",
    "    title=\"Maindeck loss for card pick by rank\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = pd.DataFrame(\n",
    "    model.test_data, columns=[\"rank_id\", \"pick_loss\", \"maindeck_loss\"]\n",
    ")\n",
    "test_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = set()\n",
    "for draft_id, draft in check.train_datasets[0].dataset.items():\n",
    "    if draft.get(\"rank\") == \"0\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686c79f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for thing, draft in check.train_datasets[0].dataset.items():\n",
    "    print(\n",
    "        [\n",
    "            i\n",
    "            for n, i in enumerate(draft[\"pick_data\"])\n",
    "            if draft[\"maindeck_rates\"][n] > 0.5\n",
    "        ]\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27048dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import MiniBatchSparsePCA, SparsePCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ArchetypeBuilder:\n",
    "    def __init__(\n",
    "        self, n_archetypes: int = 15, n_components: int = 50, alpha: float = 0.5\n",
    "    ):\n",
    "        self.n_archetypes = n_archetypes\n",
    "        self.pca = MiniBatchSparsePCA(\n",
    "            n_components=n_components, alpha=alpha, batch_size=100\n",
    "        )\n",
    "        #         self.pca = PCA(n_components=n_components)\n",
    "        #         self.pca = SparsePCA(n_components=n_components)\n",
    "        self.clusterer = MiniBatchKMeans(n_clusters=n_archetypes)\n",
    "        self.pipeline = Pipeline([(\"pca\", self.pca), (\"clustering\", self.clusterer)])\n",
    "\n",
    "    #         self.pipeline = Pipeline([(\"clustering\", self.clusterer)])\n",
    "\n",
    "    def create_sparse_played_deck_dataset(self, dataset: datasets.DraftDataset):\n",
    "        row_ids: T.List[int] = []\n",
    "        col_ids: T.List[int] = []\n",
    "        draft_ids: T.List[str] = []\n",
    "\n",
    "        for draft_id, draft in dataset.items():\n",
    "            draft_ids.append(draft_id)\n",
    "            draft_ind = len(draft_ids) - 1\n",
    "            for n, i in enumerate(draft[\"pick_data\"]):\n",
    "                if draft[\"maindeck_rates\"][n] > 0.5:\n",
    "                    row_ids.append(draft_ind)\n",
    "                    col_ids.append(i)\n",
    "        result = csr_matrix((np.ones(len(col_ids)), (row_ids, col_ids)))\n",
    "        return result\n",
    "\n",
    "    def fit(self, dataset: datasets.DraftDataset):\n",
    "        data = self.create_sparse_played_deck_dataset(dataset)\n",
    "        result = self.pipeline.fit(data.todense())\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_builder = ArchetypeBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = archetype_builder.create_sparse_played_deck_dataset(\n",
    "    check.train_datasets[0].dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a505b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_builder.fit(check.train_datasets[0].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584efe86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "card_ids_inv = {v: k for k, v in check.card_ids.items()}\n",
    "for archetype_feature in archetype_builder.pca.components_:\n",
    "    print([card_ids_inv[n] for n, i in enumerate(archetype_feature) if i > 0.05])\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06992b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_builder.clusterer.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ecc4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_ids_inv = {v: k for k, v in check.card_ids.items()}\n",
    "for archetype_feature in archetype_builder.clusterer.cluster_centers_:\n",
    "    print([card_ids_inv[n] for n, i in enumerate(archetype_feature) if i > 0.5])\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "fv = feature_vectors[: np.max(list(card_ids.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab02934",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsned_vecs = TSNE().fit_transform(fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame(tsned_vecs, columns=[\"x1\", \"x2\"]).reset_index(drop=False)\n",
    "inv_map = {v: k for k, v in card_ids.items()}\n",
    "tdf[\"card\"] = tdf[\"index\"].apply(lambda x: inv_map.get(x, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tdf, x=\"x1\", y=\"x2\", hover_data=[\"card\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0896c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
